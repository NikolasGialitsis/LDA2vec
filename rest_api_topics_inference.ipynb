{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rest_api_topics_inference.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNxrTBFpz2WqFRI+8UeMm12",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "72942b98e11c4c3197f385b763b92863": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ae9795b74b3541eb8f1971c984b3d1a0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d7549f63255e4c25bdbecfb82a74d19d",
              "IPY_MODEL_1ad0391e19274c93b306badeb0802a41"
            ]
          }
        },
        "ae9795b74b3541eb8f1971c984b3d1a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d7549f63255e4c25bdbecfb82a74d19d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b7652fa789e449e6b23727439b559c9e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_704e824fed0f48e59d7f44fdf755199c"
          }
        },
        "1ad0391e19274c93b306badeb0802a41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1ca59c07b52841c4b658c9f845917cbc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/? [00:00&lt;00:00, 12.99it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bdcc08545a5240d3993afd71079c6fdd"
          }
        },
        "b7652fa789e449e6b23727439b559c9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "704e824fed0f48e59d7f44fdf755199c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1ca59c07b52841c4b658c9f845917cbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bdcc08545a5240d3993afd71079c6fdd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NikolasGialitsis/LDA2vec/blob/master/rest_api_topics_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "me7w1oSSC-8_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "d99efd29-cde4-407a-f15a-a4b611f65c59"
      },
      "source": [
        "!pip install flask-ngrok\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: flask-ngrok in /usr/local/lib/python3.6/dist-packages (0.0.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from flask-ngrok) (2.21.0)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.6/dist-packages (from flask-ngrok) (1.1.2)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (2.8)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (2.11.2)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (7.1.1)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (1.1.0)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (1.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->Flask>=0.8->flask-ngrok) (1.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Esnk97QCNWZi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3a5c2b06-0e35-4c58-8315-db3041000a48"
      },
      "source": [
        "!git clone https://github.com/NikolasGialitsis/LDA2vec"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'LDA2vec' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIbYQP1rOIPs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 983
        },
        "outputId": "b26e077b-df73-4927-b2e5-b88503ec3727"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"LDA2vec_inference.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1szh4AuxOleuFERJPzlqehK-yPRVwka2P\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "!pip install spacy\n",
        "!pip install jellyfish\n",
        "!pip install -r /content/requirements.txt\n",
        "!pip install pylda2vec\n",
        "\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (46.1.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.38.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.6.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.21.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.6.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.4.5.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.1.0)\n",
            "Requirement already satisfied: jellyfish in /usr/local/lib/python3.6/dist-packages (0.7.2)\n",
            "Requirement already satisfied: chainer>=1.5.1 in /usr/local/lib/python3.6/dist-packages (from -r /content/requirements.txt (line 1)) (6.5.0)\n",
            "Requirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.6/dist-packages (from -r /content/requirements.txt (line 2)) (1.18.3)\n",
            "Requirement already satisfied: spacy>=0.9 in /usr/local/lib/python3.6/dist-packages (from -r /content/requirements.txt (line 3)) (2.2.4)\n",
            "Requirement already satisfied: scipy>=0.16.0 in /usr/local/lib/python3.6/dist-packages (from -r /content/requirements.txt (line 4)) (1.4.1)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from -r /content/requirements.txt (line 5)) (0.0)\n",
            "Requirement already satisfied: typing-extensions<=3.6.6 in /usr/local/lib/python3.6/dist-packages (from chainer>=1.5.1->-r /content/requirements.txt (line 1)) (3.6.6)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from chainer>=1.5.1->-r /content/requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from chainer>=1.5.1->-r /content/requirements.txt (line 1)) (46.1.3)\n",
            "Requirement already satisfied: protobuf>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from chainer>=1.5.1->-r /content/requirements.txt (line 1)) (3.10.0)\n",
            "Requirement already satisfied: typing<=3.6.6 in /usr/local/lib/python3.6/dist-packages (from chainer>=1.5.1->-r /content/requirements.txt (line 1)) (3.6.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from chainer>=1.5.1->-r /content/requirements.txt (line 1)) (3.0.12)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=0.9->-r /content/requirements.txt (line 3)) (0.4.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=0.9->-r /content/requirements.txt (line 3)) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=0.9->-r /content/requirements.txt (line 3)) (2.0.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=0.9->-r /content/requirements.txt (line 3)) (7.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=0.9->-r /content/requirements.txt (line 3)) (1.0.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=0.9->-r /content/requirements.txt (line 3)) (4.38.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=0.9->-r /content/requirements.txt (line 3)) (1.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=0.9->-r /content/requirements.txt (line 3)) (2.21.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=0.9->-r /content/requirements.txt (line 3)) (1.1.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=0.9->-r /content/requirements.txt (line 3)) (3.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=0.9->-r /content/requirements.txt (line 3)) (0.6.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn->-r /content/requirements.txt (line 5)) (0.22.2.post1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=0.9->-r /content/requirements.txt (line 3)) (1.6.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=0.9->-r /content/requirements.txt (line 3)) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=0.9->-r /content/requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=0.9->-r /content/requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=0.9->-r /content/requirements.txt (line 3)) (2020.4.5.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn->-r /content/requirements.txt (line 5)) (0.14.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=0.9->-r /content/requirements.txt (line 3)) (3.1.0)\n",
            "Requirement already satisfied: pylda2vec in /usr/local/lib/python3.6/dist-packages (1.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kaZ4xm5OZ7p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cd LDA2vec/\n",
        "!python -m spacy download en"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPNDjwNLNKqF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "db18e15e-aadc-48d8-c04b-f93f4a04c35b"
      },
      "source": [
        "\n",
        "# Commented out IPython magic to ensure Python compatibility.\n",
        "%cd /content/LDA2vec/lda2vec\n",
        "import os\n",
        "import os.path\n",
        "import pickle\n",
        "import time\n",
        "import shelve\n",
        "\n",
        "import chainer\n",
        "from chainer import cuda\n",
        "from chainer import serializers\n",
        "import chainer.optimizers as O\n",
        "import numpy as np\n",
        "\n",
        "from lda2vec import utils\n",
        "from lda2vec import prepare_topics, print_top_words_per_topic, topic_coherence\n",
        "from lda2vec import LDA2Vec\n",
        "from lda2vec import preprocess, Corpus\n",
        "#changed Preprocess.py line 'nlp = spacy.load(\"en_core_web_sm\")' to solve error en not found\n",
        "#and removed the import ... as en line \n",
        "\n",
        "import logging\n",
        "logging.basicConfig()\n",
        "import pickle\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import numpy as np\n",
        "from lda2vec import preprocess, Corpus\n",
        "\n",
        "gpu_id = int(os.getenv('CUDA_GPU', 0))\n",
        "cuda.get_device(gpu_id).use()\n",
        "print(\"Using GPU:\" + str(gpu_id))\n",
        "\n",
        "import nltk \n",
        "import string\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import json\n",
        "\n",
        "\n",
        "\n",
        "def getTopicComposition(data):\n",
        "  #with open('/content/LDA2vec/new_document.json') as f:\n",
        "  titles = []\n",
        "  abstracts = []\n",
        "  concats = []\n",
        "  concat = data.split('------------------------------')  \n",
        "  title = concat[0]\n",
        "  abstract = concat[1]\n",
        "  print('title:',title)\n",
        "  print('abstract',abstract[:100])\n",
        "  concats.append(concat[0] + ' ' + concat[1])\n",
        "  titles.append(title)\n",
        "  abstracts.append(abstract)\n",
        "\n",
        "  stop_words = set(stopwords.words('english')) \n",
        "  new_text = []\n",
        "  for text in concats.copy():\n",
        "    word_tokens = word_tokenize(text) \n",
        "    #remove stopwords\n",
        "    text = ' '.join([w.lower() for w in word_tokens if not w.lower() in stop_words and len(w) >= 3]) \n",
        "    #remove punctuation\n",
        "    #bioclean    = lambda t: re.sub('[.,?;*!%^&_+():-\\[\\]{}]', '', t.replace('\"', '').replace('/', '').replace('\\\\', '').replace(\"'\", '').strip().lower()).split()\n",
        "\n",
        "    text = ''.join([w for w in text if w not in string.punctuation])\n",
        "    new_text.append(text) \n",
        "\n",
        "  max_length = 10000\n",
        "  tokens , vocab = preprocess.tokenize(new_text, max_length, merge=False,n_threads=4)\n",
        "\n",
        "  corpus = Corpus()\n",
        "  # Make a ranked list of rare vs frequent words\n",
        "  corpus.update_word_count(tokens)\n",
        "  corpus.finalize()\n",
        "  # The tokenization uses spaCy indices, and so may have gaps\n",
        "  # between indices for words that aren't present in our dataset.\n",
        "  # This builds a new compact index\n",
        "  compact = corpus.to_compact(tokens)\n",
        "  # Remove extremely rare words\n",
        "  pruned = corpus.filter_count(compact, min_count=0)\n",
        "  # Convert the compactified arrays into bag of words arrays\n",
        "  bow = corpus.compact_to_bow(pruned)\n",
        "  # Words tend to have power law frequency, so selectively\n",
        "  # downsample the most prevalent words\n",
        "  clean = corpus.subsample_frequent(pruned)\n",
        "  # Now flatten a 2D array of document per row and word position\n",
        "  # per column to a 1D array of words. This will also remove skips\n",
        "  # and OoV words\n",
        "  doc_ids = np.arange(pruned.shape[0])\n",
        "  flattened, (doc_ids,) = corpus.compact_to_flat(pruned, doc_ids)\n",
        "\n",
        "  # Commented out IPython magic to ensure Python compatibility.\n",
        "  # %cd /content\n",
        "\n",
        "  fn_weights = '/content/LDA2vec/weights.pkl'\n",
        "  fn_sampler = '/content/LDA2vec/sampler.pkl'\n",
        "  fn_factors = '/content/LDA2vec/factors.pkl'\n",
        "  weights = pickle.load(open(fn_weights, 'rb'))\n",
        "  sampler = pickle.load(open(fn_sampler, 'rb'))\n",
        "  factors = pickle.load(open(fn_factors, 'rb'))\n",
        "  #vocab = pickle.load(open('vocab.pkl', 'rb'))\n",
        "\n",
        "  fn_corpus = '/content/LDA2vec/corpus.pkl'\n",
        "  fn_flatnd = '/content/LDA2vec/flattened.npy'\n",
        "  fn_docids = '/content/LDA2vec/doc_ids.npy'\n",
        "\n",
        "  # Model Parameters\n",
        "  # Number of documents\n",
        "  n_docs = doc_ids.max() + 1\n",
        "  # Number of unique words in the vocabulary\n",
        "  n_vocab = flattened.max() + 1\n",
        "  # 'Strength' of the dircihlet prior; 200.0 seems to work well\n",
        "  clambda = 200.0\n",
        "  # Number of topics to fit\n",
        "  n_topics = int(os.getenv('n_topics', 10))\n",
        "  batchsize = 16\n",
        "  # Power for neg sampling\n",
        "  power = float(os.getenv('power', 0.75))\n",
        "  # Intialize with pretrained word vectors\n",
        "  pretrained = bool(int(os.getenv('pretrained', True)))\n",
        "  # Sampling temperature\n",
        "  temperature = float(os.getenv('temperature', 1.0))\n",
        "  # Number of dimensions in a single word vector\n",
        "  n_units = int(os.getenv('n_units', 30))\n",
        "  # Get the string representation for every compact key\n",
        "  words = corpus.word_list(vocab)[:n_vocab]\n",
        "  print(words)\n",
        "  # How many tokens are in each document\n",
        "  doc_idx, lengths = np.unique(doc_ids, return_counts=True)\n",
        "  doc_lengths = np.zeros(doc_ids.max() + 1, dtype='int32')\n",
        "  doc_lengths[doc_idx] = lengths\n",
        "\n",
        "  # Count all token frequencies\n",
        "  tok_idx, freq = np.unique(flattened, return_counts=True)\n",
        "  term_frequency = np.zeros(n_vocab, dtype='int32')\n",
        "  term_frequency[tok_idx] = freq\n",
        "\n",
        "  model = LDA2Vec(n_documents=n_docs, n_document_topics=n_topics,\n",
        "                  n_units=n_units, n_vocab=n_vocab, counts=term_frequency,\n",
        "                  n_samples=15, power=power, temperature=temperature)\n",
        "  #vectors = np.load('vectors.npy')\n",
        "  #model.sampler.W.data[:, :] =  vectors[:n_vocab, :]\n",
        "\n",
        "  #vocab = pickle.load(open(fn_vocab, 'rb'))\n",
        "  corpus = pickle.load(open(fn_corpus, 'rb'))\n",
        "  old_flattened = np.load(fn_flatnd)\n",
        "  n_vocab = old_flattened.max() + 1\n",
        "  words = corpus.word_list(vocab)[:n_vocab]\n",
        "\n",
        "\n",
        "\n",
        "  #model.sampler.W.data[:, :] = new_text[:n_vocab, :]\n",
        "\n",
        "  data = prepare_topics(cuda.to_cpu(weights.copy()),\n",
        "                        cuda.to_cpu(factors.copy()),\n",
        "                        cuda.to_cpu(sampler.copy()),\n",
        "                        words)\n",
        "  #add new randomized vector for new doc\n",
        "\n",
        "  random_doc_topic = np.random.dirichlet(np.ones(10),size=1)\n",
        "  first_doc = data['doc_topic_dists'][0]\n",
        "\n",
        "  num_docs = data['doc_topic_dists'].shape[0]\n",
        "  data['doc_topic_dists'] = np.append(arr=data['doc_topic_dists'],values=random_doc_topic)\n",
        "  data['doc_topic_dists'] = data['doc_topic_dists'].reshape(num_docs+1,n_topics)\n",
        "\n",
        "\n",
        "  print(data['doc_topic_dists'].shape)\n",
        "  print('random_doc = ',random_doc_topic)\n",
        "\n",
        "  model.to_gpu()\n",
        "  optimizer = O.Adam()\n",
        "  optimizer.setup(model)\n",
        "  clip = chainer.optimizer.GradientClipping(5.0)\n",
        "  optimizer.add_hook(clip)\n",
        "  j = 0\n",
        "  epoch = 0\n",
        "  fraction = batchsize * 1.0 / flattened.shape[0]\n",
        "  progress = shelve.open('progress.shelve')\n",
        "\n",
        "  n_epochs = 1500\n",
        "\n",
        "  for epoch in range(1):\n",
        "      data = prepare_topics(cuda.to_cpu(weights.copy()),\n",
        "                            cuda.to_cpu(factors.copy()),\n",
        "                            cuda.to_cpu(sampler.copy()),\n",
        "                            words)\n",
        "      #top_words = print_top_words_per_topic(data)\n",
        "      data['doc_lengths'] = doc_lengths\n",
        "      data['term_frequency'] = term_frequency\n",
        "      np.savez('topics.pyldavis', **data)\n",
        "      #print(data)\n",
        "      for d, f in utils.chunks(batchsize, np.array(doc_ids), flattened):\n",
        "          t0 = time.time()\n",
        "          model.cleargrads()\n",
        "          #optimizer.use_cleargrads(use=False)\n",
        "          l = model.fit_partial(d.copy(), f.copy(),update_only_docs=True)\n",
        "          print(l)\n",
        "          print(\"after partial fitting:\", l)\n",
        "          prior = model.prior()\n",
        "          loss = prior * fraction\n",
        "          loss.backward()\n",
        "          optimizer.update()\n",
        "          msg = (\"J:{j:05d} E:{epoch:05d} L:{loss:1.3e} \"\n",
        "                \"P:{prior:1.3e} R:{rate:1.3e}\")\n",
        "          prior.to_cpu()\n",
        "          loss.to_cpu()\n",
        "          t1 = time.time()\n",
        "          dt = t1 - t0\n",
        "          rate = batchsize / dt\n",
        "          logs = dict(loss=float(l), epoch=epoch, j=j,\n",
        "                      prior=float(prior.data), rate=rate)\n",
        "          print(msg.format(**logs))\n",
        "          j += 1\n",
        "\n",
        "  print('==== MAKE SURE THAT THE OTHER DOCS DONT CHANGE')\n",
        "  print(first_doc)\n",
        "  print(data['doc_topic_dists'][0])\n",
        "  print('\\n==================================')\n",
        "  print('==== MAKE SURE THAT THE NEW DOC HAS CHANGED')\n",
        "  print(random_doc_topic)\n",
        "  print(data['doc_topic_dists'][-1])\n",
        "\n",
        "  t=0\n",
        "  print('New Document Composition\\n')\n",
        "  for d in data['doc_topic_dists'][-1]:\n",
        "    print('\\t',round(d*100,2),'\\t percent of Topic',t)\n",
        "    t+=1\n",
        "  return data['doc_topic_dists'][-1]\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/LDA2vec/lda2vec\n",
            "Using GPU:0\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oDRBA1rFaCq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 905,
          "referenced_widgets": [
            "72942b98e11c4c3197f385b763b92863",
            "ae9795b74b3541eb8f1971c984b3d1a0",
            "d7549f63255e4c25bdbecfb82a74d19d",
            "1ad0391e19274c93b306badeb0802a41",
            "b7652fa789e449e6b23727439b559c9e",
            "704e824fed0f48e59d7f44fdf755199c",
            "1ca59c07b52841c4b658c9f845917cbc",
            "bdcc08545a5240d3993afd71079c6fdd"
          ]
        },
        "outputId": "d582dea0-d86d-493e-da77-6e69adb49906"
      },
      "source": [
        "from flask_ngrok import run_with_ngrok\n",
        "from flask import Flask\n",
        "from flask import Flask, jsonify\n",
        "from flask import abort\n",
        "from flask import make_response\n",
        "from flask import request\n",
        "\n",
        "app = Flask(__name__)\n",
        "run_with_ngrok(app)\n",
        "tasks = [\n",
        "    {\n",
        "        'id': 1,\n",
        "        'title': u'Buy groceries',\n",
        "        'description': u'Milk, Cheese, Pizza, Fruit, Tylenol', \n",
        "        'done': False\n",
        "    },\n",
        "    {\n",
        "        'id': 2,\n",
        "        'title': u'Learn Python',\n",
        "        'description': u'Need to find a good Python tutorial on the web', \n",
        "        'done': False\n",
        "    }\n",
        "]\n",
        "\n",
        "\n",
        "@app.route(\"/\")\n",
        "def home():\n",
        "    return \"<h1>Infer Topic Compositions on Google Colab!</h1>\"\n",
        "\n",
        "@app.errorhandler(404)\n",
        "def not_found(error):\n",
        "    return make_response(jsonify({'error': 'Not found'}), 404)\n",
        "\n",
        "@app.route('/todo/api/v1.0/tasks', methods=['POST'])\n",
        "def create_task():\n",
        "    if not request.json or not 'title' in request.json:\n",
        "        abort(400)\n",
        "    task = {\n",
        "        'id': tasks[-1]['id'] + 1,\n",
        "        'title': request.json['title'],\n",
        "        'description': request.json.get('description', \"\"),\n",
        "        'done': False\n",
        "    }\n",
        "    print(task['title'])\n",
        "    composition = getTopicComposition(task['title'])\n",
        "    print(composition) \n",
        "    return jsonify({'topic composition': str(composition)}), 201\n",
        "\n",
        "app.run()\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:werkzeug: * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://64ce50df.ngrok.io\n",
            " * Traffic stats available on http://127.0.0.1:4040\n",
            "A covid19 paper ------------------------------ Covid19 is a disease caused by the coronavirus SARS-COV2 and scientists from the united states have tried to develop vaccine\n",
            "title: A covid19 paper \n",
            "abstract  Covid19 is a disease caused by the coronavirus SARS-COV2 and scientists from the united states have\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "72942b98e11c4c3197f385b763b92863",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [27/Apr/2020 12:03:28] \"\u001b[37mPOST /todo/api/v1.0/tasks HTTP/1.1\u001b[0m\" 201 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "['out_of_vocabulary', 'out_of_vocabulary', '<SKIP>', 'covid19', 'vaccine', 'develop', 'tried', 'states', 'united', 'scientists', 'sarscov2', 'coronavirus', 'caused', 'disease', 'paper']\n",
            "(745, 10)\n",
            "random_doc =  [[0.04585225 0.27356406 0.09677201 0.09151243 0.10756527 0.19214536\n",
            "  0.04543014 0.01054596 0.11823917 0.01837336]]\n",
            "359.89133\n",
            "after partial fitting: 359.89133\n",
            "J:00000 E:00000 L:3.599e+02 P:-2.100e+01 R:3.500e+02\n",
            "==== MAKE SURE THAT THE OTHER DOCS DONT CHANGE\n",
            "[0.10384709 0.08276632 0.10338711 0.07900679 0.1079841  0.11230687\n",
            " 0.09587169 0.10159854 0.1013382  0.11189319]\n",
            "[0.10384709 0.08276632 0.10338711 0.07900679 0.1079841  0.11230687\n",
            " 0.09587169 0.10159854 0.1013382  0.11189319]\n",
            "\n",
            "==================================\n",
            "==== MAKE SURE THAT THE NEW DOC HAS CHANGED\n",
            "[[0.04585225 0.27356406 0.09677201 0.09151243 0.10756527 0.19214536\n",
            "  0.04543014 0.01054596 0.11823917 0.01837336]]\n",
            "[0.09601615 0.09627663 0.10512727 0.09975246 0.11829065 0.09382185\n",
            " 0.09439588 0.10970131 0.08970814 0.09690975]\n",
            "New Document Composition\n",
            "\n",
            "\t 9.6 \t percent of Topic 0\n",
            "\t 9.63 \t percent of Topic 1\n",
            "\t 10.51 \t percent of Topic 2\n",
            "\t 9.98 \t percent of Topic 3\n",
            "\t 11.83 \t percent of Topic 4\n",
            "\t 9.38 \t percent of Topic 5\n",
            "\t 9.44 \t percent of Topic 6\n",
            "\t 10.97 \t percent of Topic 7\n",
            "\t 8.97 \t percent of Topic 8\n",
            "\t 9.69 \t percent of Topic 9\n",
            "[0.09601615 0.09627663 0.10512727 0.09975246 0.11829065 0.09382185\n",
            " 0.09439588 0.10970131 0.08970814 0.09690975]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}